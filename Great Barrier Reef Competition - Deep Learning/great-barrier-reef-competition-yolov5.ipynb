{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dependencies\n%cd /kaggle/input/norfair031py3/\n!pip install commonmark-0.9.1-py2.py3-none-any.whl -f ./ --no-index\n!pip install rich-9.13.0-py3-none-any.whl\n\n!mkdir /kaggle/working/tmp\n!cp -r /kaggle/input/norfair031py3/filterpy-1.4.5/filterpy-1.4.5/ /kaggle/working/tmp/\n%cd /kaggle/working/tmp/filterpy-1.4.5/\n!pip install .\n!rm -rf /kaggle/working/tmp\n\n%cd /kaggle/input/norfair031py3/\n!pip install norfair-0.3.1-py3-none-any.whl -f ./ --no-index\n%cd /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:22.14502Z","iopub.execute_input":"2022-02-01T07:20:22.145188Z","iopub.status.idle":"2022-02-01T07:20:53.406915Z","shell.execute_reply.started":"2022-02-01T07:20:22.145166Z","shell.execute_reply":"2022-02-01T07:20:53.40531Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(font_scale=1.1, rc={\n    'figure.figsize': (15, 10),\n    'axes.facecolor': 'white',\n    'figure.facecolor': 'white',\n    'grid.color': '#dddddd',\n    'grid.linewidth': 0.5,\n    \"lines.linewidth\": 1.5,\n    'text.color': '#333333',\n    'xtick.color': '#666666',\n    'ytick.color': '#666666'\n})\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nimport cv2\n\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\n\nimport sys\nsys.path.append('../input/tensorflow-great-barrier-reef')\nimport os\nimport random\nfrom pathlib import Path\nimport ast\nfrom norfair import Detection, Tracker","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.408109Z","iopub.execute_input":"2022-02-01T07:20:53.408295Z","iopub.status.idle":"2022-02-01T07:20:53.46917Z","shell.execute_reply.started":"2022-02-01T07:20:53.408271Z","shell.execute_reply":"2022-02-01T07:20:53.468038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import greatbarrierreef\nenv = greatbarrierreef.make_env()# initialize the environment\niter_test = env.iter_test()      # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.471306Z","iopub.execute_input":"2022-02-01T07:20:53.471677Z","iopub.status.idle":"2022-02-01T07:20:53.502669Z","shell.execute_reply.started":"2022-02-01T07:20:53.471651Z","shell.execute_reply":"2022-02-01T07:20:53.501711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 42\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    root_dir = '/kaggle/input/tensorflow-great-barrier-reef/'\n\n    # Info Utils\n    verbose = 1\n    plot_fold_training = True\n\n    # Number of Training Folds\n    folds = 4\n\n    # Dataset Setup\n    gbuckets = False\n    tfrecords = False\n\n    if tfrecords:\n        sample_per_shard = 100\n\n    ## Set Paths\n    train_path = '/'\n    test_path = '/' \n\n    train_meta = '../input/tensorflow-great-barrier-reef/test.csv'\n    test_meta = '../input/tensorflow-great-barrier-reef/test.csv'\n\n    if gbuckets:\n        gbuckets_dir = 'gs://.../'\n    if tfrecords:\n        train_path = gbuckets_dir + 'train/*.tfrecords'\n        test_path = gbuckets_dir + 'test/*.tfrecords'\n\n    ## Set Number of Training Items\n    if not gbuckets and not tfrecords:\n        train_count = len(next(os.walk(train_path))[2])\n        test_count = len(next(os.walk(test_path))[2])\n    elif gbuckets and not tfrecords:\n        train_count = len(tf.io.gfile.glob(train_path))\n        test_count = len(tf.io.gfile.glob(test_path))\n    else:\n        ### Manually add if using tfrecords\n        train_count = 100\n        test_count = 100\n\n    # Image Info (Width, Height)\n    img_size = [1280, 720]\n\n    # Training Parameters\n    batch_size = 32\n    epochs = 10\n    steps_per_epoch = train_count // batch_size // 4\n    aug = False\n\n\n\n    model_dir = '/content/drive/MyDrive/my_model/'\n    model_name = 'EfficientNetB7'","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.505439Z","iopub.execute_input":"2022-02-01T07:20:53.505738Z","iopub.status.idle":"2022-02-01T07:20:53.514281Z","shell.execute_reply.started":"2022-02-01T07:20:53.505712Z","shell.execute_reply":"2022-02-01T07:20:53.513836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = cv2.cvtColor(cv2.imread('../input/tensorflow-great-barrier-reef/train_images/video_0/0.jpg'), cv2.COLOR_RGB2BGR)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.51643Z","iopub.execute_input":"2022-02-01T07:20:53.517214Z","iopub.status.idle":"2022-02-01T07:20:53.595749Z","shell.execute_reply.started":"2022-02-01T07:20:53.517176Z","shell.execute_reply":"2022-02-01T07:20:53.595243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(test_img)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.597982Z","iopub.execute_input":"2022-02-01T07:20:53.598443Z","iopub.status.idle":"2022-02-01T07:20:53.797777Z","shell.execute_reply.started":"2022-02-01T07:20:53.598416Z","shell.execute_reply":"2022-02-01T07:20:53.797089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir -p /root/.config/Ultralytics\n!cp /kaggle/input/yolov5-font/Arial.ttf /root/.config/Ultralytics/","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:53.798744Z","iopub.execute_input":"2022-02-01T07:20:53.798972Z","iopub.status.idle":"2022-02-01T07:20:54.410589Z","shell.execute_reply.started":"2022-02-01T07:20:53.798942Z","shell.execute_reply":"2022-02-01T07:20:54.409456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if TRACKING:\ndef to_norfair(detects, frame_id):\n    result = []\n    for x_min, y_min, x_max, y_max, score in detects:\n        xc, yc = (x_min + x_max) / 2, (y_min + y_max) / 2\n        w, h = x_max - x_min, y_max - y_min\n        result.append(Detection(points=np.array([xc, yc]), scores=np.array([score]), data=np.array([w, h, frame_id])))\n\n    return result\n\n# Euclidean distance function to match detections on this frame with tracked_objects from previous frames\ndef euclidean_distance(detection, tracked_object):\n    return np.linalg.norm(detection.points - tracked_object.estimate)\n\ntracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\nframe_id = 0","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-02-01T07:20:54.412122Z","iopub.execute_input":"2022-02-01T07:20:54.412507Z","iopub.status.idle":"2022-02-01T07:20:54.420295Z","shell.execute_reply.started":"2022-02-01T07:20:54.412471Z","shell.execute_reply":"2022-02-01T07:20:54.419833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Appending Image Paths to Image Ids","metadata":{}},{"cell_type":"code","source":"def get_path(row):\n    row['image_path'] = f'{CFG.root_dir}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:54.422898Z","iopub.execute_input":"2022-02-01T07:20:54.423184Z","iopub.status.idle":"2022-02-01T07:20:54.445109Z","shell.execute_reply.started":"2022-02-01T07:20:54.423149Z","shell.execute_reply":"2022-02-01T07:20:54.444502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(f'{CFG.root_dir}/train.csv')\ndf = df.progress_apply(get_path, axis=1)\ndf['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:20:54.446039Z","iopub.execute_input":"2022-02-01T07:20:54.446454Z","iopub.status.idle":"2022-02-01T07:21:08.695004Z","shell.execute_reply.started":"2022-02-01T07:20:54.446419Z","shell.execute_reply":"2022-02-01T07:21:08.694516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of BBoxes in Dataset","metadata":{}},{"cell_type":"code","source":"df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\ndata = (df.num_bbox>0).value_counts()/len(df)*100\nprint(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.69587Z","iopub.execute_input":"2022-02-01T07:21:08.696435Z","iopub.status.idle":"2022-02-01T07:21:08.769215Z","shell.execute_reply.started":"2022-02-01T07:21:08.696397Z","shell.execute_reply":"2022-02-01T07:21:08.768351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## BBoxes Utils","metadata":{}},{"cell_type":"code","source":"def voc2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    voc  => [x1, y1, x2, y1]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n    \n    w = bboxes[..., 2] - bboxes[..., 0]\n    h = bboxes[..., 3] - bboxes[..., 1]\n    \n    bboxes[..., 0] = bboxes[..., 0] + w/2\n    bboxes[..., 1] = bboxes[..., 1] + h/2\n    bboxes[..., 2] = w\n    bboxes[..., 3] = h\n    \n    return bboxes\n\ndef yolo2voc(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n    \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n    \n    return bboxes\n\ndef coco2yolo(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    coco => [xmin, ymin, w, h]\n    yolo => [xmid, ymid, w, h] (normalized)\n    \"\"\"\n    \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # normolizinig\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n    \n    # converstion (xmin, ymin) => (xmid, ymid)\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef yolo2coco(bboxes, image_height=720, image_width=1280):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    coco => [xmin, ymin, w, h]\n    \n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n    \n    # denormalizing\n    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n    \n    # converstion (xmid, ymid) => (xmin, ymin) \n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n    \n    return bboxes\n\ndef voc2coco(bboxes, image_height=720, image_width=1280):\n    bboxes  = voc2yolo(bboxes, image_height, image_width)\n    bboxes  = yolo2coco(bboxes, image_height, image_width)\n    return bboxes\n\ndef load_image(image_path):\n    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n\n\ndef plot_one_box(x, img, color=None, label=None, line_thickness=None):\n    # Plots one bounding box on image img\n    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n    color = color or [random.randint(0, 255) for _ in range(3)]\n    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n    if label:\n        tf = max(tl - 1, 1)  # font thickness\n        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n\ndef draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n     \n    image = img.copy()\n    show_classes = classes if show_classes is None else show_classes\n    colors = (0, 255 ,0) if colors is None else colors\n    \n    if bbox_format == 'yolo':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:\n            \n                x1 = round(float(bbox[0])*image.shape[1])\n                y1 = round(float(bbox[1])*image.shape[0])\n                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n                h  = round(float(bbox[3])*image.shape[0]/2)\n\n                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(get_label(cls)),\n                             line_thickness = line_thickness)\n            \n    elif bbox_format == 'coco':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes:            \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                w  = int(round(bbox[2]))\n                h  = int(round(bbox[3]))\n\n                voc_bbox = (x1, y1, x1+w, y1+h)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n\n    elif bbox_format == 'voc_pascal':\n        \n        for idx in range(len(bboxes)):  \n            \n            bbox  = bboxes[idx]\n            cls   = classes[idx]\n            cls_id = class_ids[idx]\n            color = colors[cls_id] if type(colors) is list else colors\n            \n            if cls in show_classes: \n                x1 = int(round(bbox[0]))\n                y1 = int(round(bbox[1]))\n                x2 = int(round(bbox[2]))\n                y2 = int(round(bbox[3]))\n                voc_bbox = (x1, y1, x2, y2)\n                plot_one_box(voc_bbox, \n                             image,\n                             color = color,\n                             label = cls if class_name else str(cls_id),\n                             line_thickness = line_thickness)\n    else:\n        raise ValueError('wrong bbox format')\n\n    return image\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_imgsize(row):\n    row['width'], row['height'] = imagesize.get(row['image_path'])\n    return row","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.770364Z","iopub.execute_input":"2022-02-01T07:21:08.770523Z","iopub.status.idle":"2022-02-01T07:21:08.800037Z","shell.execute_reply.started":"2022-02-01T07:21:08.770501Z","shell.execute_reply":"2022-02-01T07:21:08.799348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Inference Utils","metadata":{}},{"cell_type":"code","source":"def predict(model, img, size=768, augment=False):\n    height, width = img.shape[:2]\n    results = model(img, size=size, augment=augment)  # custom inference size\n    preds   = results.pandas().xyxy[0]\n    bboxes  = preds[['xmin','ymin','xmax','ymax']].values\n    if len(bboxes):\n        bboxes  = voc2coco(bboxes,height,width).astype(int)\n        confs   = preds.confidence.values\n        return bboxes, confs\n    else:\n        return [],[]\n    \ndef format_prediction(bboxes, confs):\n    annot = ''\n    if len(bboxes)>0:\n        for idx in range(len(bboxes)):\n            xmin, ymin, w, h = bboxes[idx]\n            conf             = confs[idx]\n            annot += f'{conf} {xmin} {ymin} {w} {h}'\n            annot +=' '\n        annot = annot.strip(' ')\n    return annot","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.801206Z","iopub.execute_input":"2022-02-01T07:21:08.80156Z","iopub.status.idle":"2022-02-01T07:21:08.812731Z","shell.execute_reply.started":"2022-02-01T07:21:08.801534Z","shell.execute_reply":"2022-02-01T07:21:08.811789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tracking_function(tracker, frame_id, bboxes, scores):\n    \n    detects = []\n    predictions = []\n    \n    if len(scores)>0:\n        for i in range(len(bboxes)):\n            box = bboxes[i]\n            score = scores[i]\n            x_min = int(box[0])\n            y_min = int(box[1])\n            bbox_width = int(box[2])\n            bbox_height = int(box[3])\n            detects.append([x_min, y_min, x_min+bbox_width, y_min+bbox_height, score])\n            predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n#             print(predictions[:-1])\n    # Update tracks using detects from current frame\n    tracked_objects = tracker.update(detections=to_norfair(detects, frame_id))\n    for tobj in tracked_objects:\n        bbox_width, bbox_height, last_detected_frame_id = tobj.last_detection.data\n        if last_detected_frame_id == frame_id:  # Skip objects that were detected on current frame\n            continue\n        # Add objects that have no detections on current frame to predictions\n        xc, yc = tobj.estimate[0]\n        x_min, y_min = int(round(xc - bbox_width / 2)), int(round(yc - bbox_height / 2))\n        score = tobj.last_detection.scores[0]\n\n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n        \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.813894Z","iopub.execute_input":"2022-02-01T07:21:08.814072Z","iopub.status.idle":"2022-02-01T07:21:08.832294Z","shell.execute_reply.started":"2022-02-01T07:21:08.814045Z","shell.execute_reply":"2022-02-01T07:21:08.831787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(ckpt_path, conf=0.25, iou=0.50):\n    model = torch.hub.load('../input/yolov5-lib-ds', \n                       'custom', \n                       path='../input/yolos-video-fold/yolov5s6_3000_b6_uflip_split91_f1_v12.pt',\n                       \n#                        path='../input/yolos-video-fold/2022_yolos_3600_f1_best.pt',\n                       source='local',\n                       force_reload=True)  # local repo\n    model.conf = CONF\n    model.conf = conf  # NMS confidence threshold\n    model.iou  = iou   # NMS IoU threshold\n    model.classes = None   # (optional list) filter by class, i.e. = [0, 15, 16] for persons, cats and dogs\n    model.multi_label = False  # NMS multiple labels per box\n    model.max_det = 1000  # maximum number of detections per image\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.833166Z","iopub.execute_input":"2022-02-01T07:21:08.833781Z","iopub.status.idle":"2022-02-01T07:21:08.852754Z","shell.execute_reply.started":"2022-02-01T07:21:08.833715Z","shell.execute_reply":"2022-02-01T07:21:08.851886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ROOT_DIR  = '/kaggle/input/tensorflow-great-barrier-reef/'\nweights_path = '../input/yolov5-1920-4/'   # ../input/yolov5-1920-4/best.pt\nCKPT_PATH = weights_path + '/best.pt'\nIMG_SIZE  = int(2000*3)  # \nCONF      = 0.275     # 1920*3 + conf-0.3\nIOU       = 0.2\nAUGMENT   = True     # TTA will run for an hour, will gain improvement\nTRACKING  = False\nFDA_aug   = False","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.853867Z","iopub.execute_input":"2022-02-01T07:21:08.854111Z","iopub.status.idle":"2022-02-01T07:21:08.871141Z","shell.execute_reply.started":"2022-02-01T07:21:08.854078Z","shell.execute_reply":"2022-02-01T07:21:08.870664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('../input/yolov5-lib-ds', \n                       'custom', \n                       path='../input/yolos-video-fold/yolov5s6_3000_b6_uflip_split91_f1_v12.pt',\n                       \n#                        path='../input/yolos-video-fold/2022_yolos_3600_f1_best.pt',\n                       source='local',\n                       force_reload=True)  # local repo\nmodel.conf = CONF","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:08.872129Z","iopub.execute_input":"2022-02-01T07:21:08.872538Z","iopub.status.idle":"2022-02-01T07:21:09.308917Z","shell.execute_reply.started":"2022-02-01T07:21:08.872494Z","shell.execute_reply":"2022-02-01T07:21:09.308387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tracker = Tracker(\n    distance_function=euclidean_distance, \n    distance_threshold=30,\n    hit_inertia_min=3,\n    hit_inertia_max=6,\n    initialization_delay=1,\n)\n\nframe_id =0\nfor idx, (img, pred_df) in enumerate(tqdm(iter_test)):\n    if FDA_aug:\n        img = FDA_trans(image=img)['image']\n    bboxes, confs  = predict(model, img, size=IMG_SIZE, augment=AUGMENT)\n\n    predictions = tracking_function(tracker, frame_id, bboxes, confs)\n    \n    prediction_str = ' '.join(predictions)\n    pred_df['annotations'] = prediction_str\n    env.predict(pred_df)\n#     if frame_id < 3:\n#         if len(predict_box)>0:\n#             box = [list(map(int,box.split(' ')[1:])) for box in predictions]\n#         else:\n#             box = []\n#         display(show_img(img, box, bbox_format='coco'))\n#     print('Prediction:', pred_df)\n    frame_id += 1\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:21:09.310107Z","iopub.execute_input":"2022-02-01T07:21:09.311222Z","iopub.status.idle":"2022-02-01T07:22:17.070534Z","shell.execute_reply.started":"2022-02-01T07:21:09.311167Z","shell.execute_reply":"2022-02-01T07:22:17.068659Z"},"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df","metadata":{"execution":{"iopub.status.busy":"2022-02-01T07:22:24.96452Z","iopub.execute_input":"2022-02-01T07:22:24.964759Z","iopub.status.idle":"2022-02-01T07:22:24.976215Z","shell.execute_reply.started":"2022-02-01T07:22:24.964736Z","shell.execute_reply":"2022-02-01T07:22:24.975385Z"},"trusted":true},"execution_count":null,"outputs":[]}]}